# Large Dataset Optimization for Vintage Forecasting (10M+ Rows)

## Problem Statement

When working with large loan performance datasets (10M+ rows), the original approach of loading all raw data into pandas causes:

1. **Memory Issues**: 10M+ rows Ã— ~100 bytes = ~1GB+ RAM usage
2. **Performance Issues**: Slow data transfer from Snowflake to Python
3. **KeyError Issues**: Grouping operations fail due to data type inconsistencies
4. **Processing Delays**: Heavy calculations in Python instead of leveraging Snowflake's power

## Optimized Solution

### Approach: Move Calculations to Snowflake SQL

Instead of loading raw data and calculating in Python, we move the heavy lifting to Snowflake SQL:

```sql
-- BEFORE: Load 10M+ raw records
SELECT * FROM loan_performance_table

-- AFTER: Load aggregated vintage metrics
WITH cleaned_data AS (...),
     fico_bands AS (...),
     vintage_aggregates AS (...),
     vintage_metrics AS (...)
SELECT vintage_date, fico_band, seasoning_month, 
       cumulative_charge_off_flag, ...
FROM vintage_metrics
```

### Data Reduction Impact

| Data Type | Original Size | Optimized Size | Reduction |
|-----------|---------------|----------------|-----------|
| Raw Records | 10,000,000+ rows | ~50,000 rows | 99.5% |
| Memory Usage | ~1GB+ | ~10MB | 99% |
| Transfer Time | Minutes | Seconds | 90%+ |

## Key Optimizations

### 1. **Pre-Aggregated Vintage Metrics**
```python
# Instead of loading raw data and grouping in Python:
df = session.sql("SELECT * FROM loan_table").to_pandas()  # 10M+ rows
vintage_metrics = df.groupby(['vintage_date', 'fico_band', 'seasoning_month']).agg(...)

# Load pre-calculated metrics directly:
vintage_metrics = session.sql(vintage_metrics_query).to_pandas()  # ~50K rows
```

### 2. **FICO Band Assignment in SQL**
```sql
CASE 
    WHEN fico_score >= 800 THEN '800+'
    WHEN fico_score >= 750 THEN '750-799'
    WHEN fico_score >= 700 THEN '700-749'
    WHEN fico_score >= 650 THEN '650-699'
    WHEN fico_score >= 600 THEN '600-649'
    ELSE '<600'
END AS fico_band
```

### 3. **Cumulative Calculations in SQL**
```sql
SUM(total_charge_off_amount) OVER (
    PARTITION BY vintage_date, fico_band 
    ORDER BY seasoning_month 
    ROWS UNBOUNDED PRECEDING
) AS cumulative_charge_off_amount
```

### 4. **Loan ID Cleaning in SQL**
```sql
WHERE 
    loan_id IS NOT NULL 
    AND TRIM(CAST(loan_id AS STRING)) != ''
    AND TRIM(CAST(loan_id AS STRING)) != 'nan'
    AND TRIM(CAST(loan_id AS STRING)) != 'None'
```

## Implementation Files

### 1. `snowflake_optimized_queries.py`
Contains optimized SQL query generators:
- `get_optimized_vintage_metrics_query()`
- `get_optimized_loan_summary_query()`
- `get_optimized_mature_vintage_query()`
- `get_optimized_forecast_data_query()`
- `get_optimized_fico_mix_query()`

### 2. `standalone_vintage_forecast_snowflake.py` (Updated)
Modified notebook that uses optimized queries instead of loading raw data.

### 3. `snowflake_specific_fixes.py`
Handles Snowflake-specific data type issues and provides diagnostic tools.

## Usage Example

```python
from snowflake_optimized_queries import get_optimized_vintage_metrics_query

# Base query
base_query = '''
SELECT loan_id, vintage_date, seasoning_month, fico_score, 
       loan_amount, charge_off_amount, outstanding_balance
FROM your_database.your_schema.your_loan_performance_table
WHERE vintage_date >= '2018-01-01'
'''

# Get optimized vintage metrics
session = get_active_session()
vintage_metrics_query = get_optimized_vintage_metrics_query(base_query)
vintage_metrics = session.sql(vintage_metrics_query).to_pandas()

print(f"Loaded {len(vintage_metrics):,} vintage metric records")
print(f"Memory usage: ~{vintage_metrics.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB")
```

## Benefits

### 1. **Memory Efficiency**
- Reduces RAM usage by 99%
- Enables processing on machines with limited memory
- Allows for larger date ranges and more granular analysis

### 2. **Performance**
- Faster data transfer (seconds vs minutes)
- Leverages Snowflake's distributed computing power
- Reduces Python processing time

### 3. **Reliability**
- Eliminates KeyError issues from loan_id grouping
- Handles data type inconsistencies at the SQL level
- More robust error handling

### 4. **Scalability**
- Can handle 100M+ rows without memory issues
- Easy to extend for additional aggregations
- Supports real-time data updates

## Migration Guide

### From Original Approach:
```python
# OLD: Load all raw data
df = session.sql(query).to_pandas()
df['fico_band'] = df['fico_score'].apply(assign_fico_band)
vintage_metrics = df.groupby(['vintage_date', 'fico_band', 'seasoning_month']).agg(...)
```

### To Optimized Approach:
```python
# NEW: Load pre-aggregated data
vintage_metrics_query = get_optimized_vintage_metrics_query(base_query)
vintage_metrics = session.sql(vintage_metrics_query).to_pandas()
```

## Troubleshooting

### Common Issues:

1. **SQL Syntax Errors**: Check Snowflake version compatibility
2. **Memory Still High**: Ensure you're using the optimized queries, not raw data
3. **Missing Data**: Verify the base query returns expected columns
4. **Performance Issues**: Add appropriate indexes in Snowflake

### Diagnostic Tools:
```python
from snowflake_specific_fixes import snowflake_diagnostic
df = snowflake_diagnostic(your_query)
```

## Future Enhancements

1. **Materialized Views**: Create persistent aggregated tables in Snowflake
2. **Incremental Updates**: Only process new data since last run
3. **Partitioning**: Use Snowflake clustering for better performance
4. **Caching**: Cache frequently used aggregations

## Conclusion

This optimized approach transforms vintage forecasting from a memory-intensive, slow process into a fast, efficient workflow that can handle datasets of any size while maintaining accuracy and reliability. 